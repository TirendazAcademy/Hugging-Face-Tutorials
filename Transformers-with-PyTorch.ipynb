{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tirendazacademy/transformers-with-pytorch?scriptVersionId=143972845\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Transformers with PyTorch</div></b>\n![](https://pytorch.org/assets/images/pytorch-2.0-feature-img.png)\n\nThis notebook walks you through how to work with Transformers using PyTorch.","metadata":{}},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>1. Loading Dataset</div></b>","metadata":{"id":"qV3rP1-P8jtH"}},{"cell_type":"markdown","source":"Let's install the \"datasets\" library using pip. This library is commonly used for accessing and working with various datasets.","metadata":{}},{"cell_type":"code","source":"# Installing datasets:\n!pip install -q datasets","metadata":{"id":"GMBoR2S9wRwe","outputId":"26305baf-93cd-4de5-9400-6e36274d59a6","execution":{"iopub.status.busy":"2023-09-23T05:26:47.807343Z","iopub.execute_input":"2023-09-23T05:26:47.807891Z","iopub.status.idle":"2023-09-23T05:27:00.12098Z","shell.execute_reply.started":"2023-09-23T05:26:47.80785Z","shell.execute_reply":"2023-09-23T05:27:00.119242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's import the load_dataset function from the \"datasets\" library and use it to load the \"rotten_tomatoes\" dataset. ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Loading the rotten_tomatoes dataset:\ndataset = load_dataset(\"rotten_tomatoes\")","metadata":{"id":"jaXq6eNB71Bc","outputId":"86a06207-5b06-4931-a728-0fa0f7d9b71a","execution":{"iopub.status.busy":"2023-09-23T05:27:00.124105Z","iopub.execute_input":"2023-09-23T05:27:00.124444Z","iopub.status.idle":"2023-09-23T05:27:05.043101Z","shell.execute_reply.started":"2023-09-23T05:27:00.124412Z","shell.execute_reply":"2023-09-23T05:27:05.042065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>2. Model Loading</div></b>","metadata":{}},{"cell_type":"markdown","source":"Let's import the AutoModelForSequenceClassification class from the \"transformers\" library and initialize a pre-trained sequence classification model called \"distilbert-base-uncased.\"","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# Loading DistilBERT:\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")","metadata":{"id":"it8e8MJX8nEk","outputId":"ddaa5d62-2fd1-46aa-d297-8218fd919b85","execution":{"iopub.status.busy":"2023-09-23T05:27:05.044538Z","iopub.execute_input":"2023-09-23T05:27:05.045713Z","iopub.status.idle":"2023-09-23T05:27:31.134685Z","shell.execute_reply.started":"2023-09-23T05:27:05.045676Z","shell.execute_reply":"2023-09-23T05:27:31.133717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>3. Tokenization</div></b>","metadata":{}},{"cell_type":"markdown","source":"Let's first imports the AutoTokenizer class and initialize a tokenizer that corresponds to the previously initialized model.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Loading tokenizer for DistilBERT:\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"id":"cFPjAlkSEspk","execution":{"iopub.status.busy":"2023-09-23T05:27:31.222956Z","iopub.execute_input":"2023-09-23T05:27:31.22481Z","iopub.status.idle":"2023-09-23T05:27:32.293509Z","shell.execute_reply.started":"2023-09-23T05:27:31.223505Z","shell.execute_reply":"2023-09-23T05:27:32.292498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a function named tokenize_dataset. It takes a dataset as input and tokenizes the \"text\" column of the dataset using the previously created tokenizer. And then let's use to apply this tokenization function to the entire dataset in batches.","metadata":{}},{"cell_type":"code","source":"# Creating a function to tokenize the dataset:\ndef tokenize_dataset(dataset):\n    return tokenizer(dataset[\"text\"])\n\n# Applying tokenizer to all dataset:\ndataset = dataset.map(tokenize_dataset, batched=True)","metadata":{"id":"sr2MTttbEu2I","execution":{"iopub.status.busy":"2023-09-23T05:27:32.295037Z","iopub.execute_input":"2023-09-23T05:27:32.295419Z","iopub.status.idle":"2023-09-23T05:27:33.454307Z","shell.execute_reply.started":"2023-09-23T05:27:32.295387Z","shell.execute_reply":"2023-09-23T05:27:33.453031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>4. Padding</div></b>","metadata":{}},{"cell_type":"markdown","source":"Let me import the DataCollatorWithPadding class and initialize a data collator that will be used to batch and pad the tokenized data.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\n# Creating a data collator that will dynamically pad the inputs received:\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"id":"tETzBFa8LOsp","execution":{"iopub.status.busy":"2023-09-23T05:27:33.455973Z","iopub.execute_input":"2023-09-23T05:27:33.456781Z","iopub.status.idle":"2023-09-23T05:27:33.988278Z","shell.execute_reply.started":"2023-09-23T05:27:33.456742Z","shell.execute_reply":"2023-09-23T05:27:33.987041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>5. Setting Training Arguments</div></b>","metadata":{}},{"cell_type":"markdown","source":"Let's set up the training arguments, specifying details such as the output directory for model checkpoints, learning rate, batch sizes, number of training epochs, and reporting options.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\n# Creating training arguments that contain the model hyperparameters:\ntraining_args = TrainingArguments(\n    output_dir=\"my_bert_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=2,\n    report_to=\"none\",\n)","metadata":{"id":"BiADV6khEodY","outputId":"552a5c15-c10e-4d7f-f43c-7518f8ee887d","execution":{"iopub.status.busy":"2023-09-23T05:31:37.256324Z","iopub.execute_input":"2023-09-23T05:31:37.256747Z","iopub.status.idle":"2023-09-23T05:31:37.263873Z","shell.execute_reply.started":"2023-09-23T05:31:37.256713Z","shell.execute_reply":"2023-09-23T05:31:37.262316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>6. Model Training</div></b>","metadata":{}},{"cell_type":"markdown","source":"Next, let's first import the Trainer class and initialize a trainer object. This trainer is used to train the model using the specified training arguments, datasets, tokenizer, and data collator.","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\n# Gathering all these classes in Trainer:\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,   \n)","metadata":{"id":"0FeDnOYLdooZ","execution":{"iopub.status.busy":"2023-09-23T05:31:46.995847Z","iopub.execute_input":"2023-09-23T05:31:46.996226Z","iopub.status.idle":"2023-09-23T05:31:47.007657Z","shell.execute_reply.started":"2023-09-23T05:31:46.996194Z","shell.execute_reply":"2023-09-23T05:31:47.006643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's call train() to start training.","metadata":{}},{"cell_type":"code","source":"# Calling train() to start training:\ntrainer.train()","metadata":{"id":"0tO4xoCJea71","execution":{"iopub.status.busy":"2023-09-23T05:31:50.666568Z","iopub.execute_input":"2023-09-23T05:31:50.66692Z","iopub.status.idle":"2023-09-23T05:34:05.516564Z","shell.execute_reply.started":"2023-09-23T05:31:50.66689Z","shell.execute_reply":"2023-09-23T05:34:05.515588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>7. Prediction</div></b>","metadata":{}},{"cell_type":"markdown","source":"To predict, let's create a text.","metadata":{}},{"cell_type":"code","source":"# Getting a text for prediction:\ntext = \"I love NLP. It's fun to analyze the NLP tasks with Hugging Face\"","metadata":{"execution":{"iopub.status.busy":"2023-09-23T05:39:30.306414Z","iopub.execute_input":"2023-09-23T05:39:30.306815Z","iopub.status.idle":"2023-09-23T05:39:30.312257Z","shell.execute_reply.started":"2023-09-23T05:39:30.306777Z","shell.execute_reply":"2023-09-23T05:39:30.31106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's tokenize text and store in the inputs variable.","metadata":{}},{"cell_type":"code","source":"# Preprocessing the text:\ninputs = tokenizer(text, return_tensors=\"pt\")\ninputs","metadata":{"execution":{"iopub.status.busy":"2023-09-23T05:39:40.196753Z","iopub.execute_input":"2023-09-23T05:39:40.197151Z","iopub.status.idle":"2023-09-23T05:39:40.206231Z","shell.execute_reply.started":"2023-09-23T05:39:40.197118Z","shell.execute_reply":"2023-09-23T05:39:40.205063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To calculate predictions, first, let's import the PyTorch library, and load the fine-tuned model from a specified path. This fine-tuned model is used for inference.","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Loading the model from the file:\nmodel_path = \"/kaggle/working/my_bert_model/checkpoint-1000\"\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_path, num_labels=2)\n\n# Calculating predictions:\nwith torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"execution":{"iopub.status.busy":"2023-09-23T05:40:15.78672Z","iopub.execute_input":"2023-09-23T05:40:15.787904Z","iopub.status.idle":"2023-09-23T05:40:17.3061Z","shell.execute_reply.started":"2023-09-23T05:40:15.787859Z","shell.execute_reply":"2023-09-23T05:40:17.305105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the class with the highest logit score as the predicted class. The result is stored in the predicted_class_id variable. This is essentially performing a classification task where the model predicts a class label based on the input text.","metadata":{}},{"cell_type":"code","source":"# Looking the prediction:\npredicted_class_id = logits.argmax().item()\npredicted_class_id","metadata":{"execution":{"iopub.status.busy":"2023-09-23T05:40:34.757054Z","iopub.execute_input":"2023-09-23T05:40:34.757992Z","iopub.status.idle":"2023-09-23T05:40:34.775038Z","shell.execute_reply.started":"2023-09-23T05:40:34.757942Z","shell.execute_reply":"2023-09-23T05:40:34.773451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks for reading. If you enjoyed this notebook, don't forget to upvote üëç\n\nLet's connect [YouTube](http://youtube.com/tirendazacademy) | [Medium](http://tirendazacademy.medium.com) | [Twitter](http://twitter.com/tirendazacademy) | [Instagram](https://www.instagram.com/tirendazacademy) | [GitHub](http://github.com/tirendazacademy) | [Linkedin](https://www.linkedin.com/in/tirendaz-academy) | [Kaggle](https://www.kaggle.com/tirendazacademy) üòé","metadata":{}}]}