{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"rotten_tomatoes\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-24T06:11:57.583109Z","iopub.execute_input":"2023-08-24T06:11:57.583459Z","iopub.status.idle":"2023-08-24T06:12:02.437945Z","shell.execute_reply.started":"2023-08-24T06:11:57.583426Z","shell.execute_reply":"2023-08-24T06:12:02.436894Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1f5fe6cf6947cea9e88870931bd329"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/921 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f06b6b5937b041e593df73c202285faa"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset rotten_tomatoes_movie_review/default (download: 476.34 KiB, generated: 1.28 MiB, post-processed: Unknown size, total: 1.75 MiB) to /root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/488k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"985aad79ff014fba99b91651fec68c90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset rotten_tomatoes_movie_review downloaded and prepared to /root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47a36a3ebfea420d8430130b918d1e12"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:02.440197Z","iopub.execute_input":"2023-08-24T06:12:02.441426Z","iopub.status.idle":"2023-08-24T06:12:02.448941Z","shell.execute_reply.started":"2023-08-24T06:12:02.441385Z","shell.execute_reply":"2023-08-24T06:12:02.447847Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1066\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:02.450187Z","iopub.execute_input":"2023-08-24T06:12:02.451162Z","iopub.status.idle":"2023-08-24T06:12:02.463727Z","shell.execute_reply.started":"2023-08-24T06:12:02.451127Z","shell.execute_reply":"2023-08-24T06:12:02.462632Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n 'label': 1}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"markdown","source":"Let's load a DistilBERT tokenizer to preprocess the text field:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:02.465250Z","iopub.execute_input":"2023-08-24T06:12:02.465963Z","iopub.status.idle":"2023-08-24T06:12:06.428583Z","shell.execute_reply.started":"2023-08-24T06:12:02.465927Z","shell.execute_reply":"2023-08-24T06:12:06.427531Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a671c8a1a2e41b49f131fcad5d92912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bea7dc46fc8442c90e2cf62352ffbce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"357adaa1af5b40ac8b7cd749128423e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"694ee70070134c709e63f231a3f0192f"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's create a preprocessing function to tokenize text:","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:06.431940Z","iopub.execute_input":"2023-08-24T06:12:06.432712Z","iopub.status.idle":"2023-08-24T06:12:06.437335Z","shell.execute_reply.started":"2023-08-24T06:12:06.432683Z","shell.execute_reply":"2023-08-24T06:12:06.436442Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Let's apply this function to our dataset with the `map` method:","metadata":{}},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:06.439139Z","iopub.execute_input":"2023-08-24T06:12:06.440253Z","iopub.status.idle":"2023-08-24T06:12:07.581284Z","shell.execute_reply.started":"2023-08-24T06:12:06.440218Z","shell.execute_reply":"2023-08-24T06:12:07.580163Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b0d2fc5b5d427f90ced12fca8c6f3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f643c7a6bc4a0eb76ad83b160072e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce3677b616b44dee8275036b26a442e1"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's use DataCollatorWithPadding to perform dynamically pad:","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:07.583047Z","iopub.execute_input":"2023-08-24T06:12:07.583450Z","iopub.status.idle":"2023-08-24T06:12:20.783945Z","shell.execute_reply.started":"2023-08-24T06:12:07.583409Z","shell.execute_reply":"2023-08-24T06:12:20.782596Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"markdown","source":"Let's use Evaluate library to evaluate the model using the accuracy metric:Let's create a function to calculate the accuracy:","metadata":{}},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:20.785810Z","iopub.execute_input":"2023-08-24T06:12:20.786694Z","iopub.status.idle":"2023-08-24T06:12:33.879575Z","shell.execute_reply.started":"2023-08-24T06:12:20.786656Z","shell.execute_reply":"2023-08-24T06:12:33.878373Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\n\naccuracy = evaluate.load(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:33.882438Z","iopub.execute_input":"2023-08-24T06:12:33.882818Z","iopub.status.idle":"2023-08-24T06:12:37.554829Z","shell.execute_reply.started":"2023-08-24T06:12:33.882779Z","shell.execute_reply":"2023-08-24T06:12:37.553865Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"034cf9937c374e20b9ae38624f434a08"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's create a function to calculate the accuracy:","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:37.556519Z","iopub.execute_input":"2023-08-24T06:12:37.556933Z","iopub.status.idle":"2023-08-24T06:12:37.564195Z","shell.execute_reply.started":"2023-08-24T06:12:37.556852Z","shell.execute_reply":"2023-08-24T06:12:37.562894Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"markdown","source":"First, let's create a map of the expected ids to their labels with id2label and label2id:","metadata":{}},{"cell_type":"code","source":"id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\nlabel2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:37.568035Z","iopub.execute_input":"2023-08-24T06:12:37.568822Z","iopub.status.idle":"2023-08-24T06:12:37.578624Z","shell.execute_reply.started":"2023-08-24T06:12:37.568786Z","shell.execute_reply":"2023-08-24T06:12:37.577700Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Let's load DistilBERT with AutoModelForSequenceClassification along with the number of expected labels, and the label mappings:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:37.580199Z","iopub.execute_input":"2023-08-24T06:12:37.580580Z","iopub.status.idle":"2023-08-24T06:12:39.646305Z","shell.execute_reply.started":"2023-08-24T06:12:37.580548Z","shell.execute_reply":"2023-08-24T06:12:39.645425Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"656923dcab0e4ef098a7f03ce1c4aa87"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's define your training hyperparameters in TrainingArguments:","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"my_great_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n    report_to = \"none\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:39.647926Z","iopub.execute_input":"2023-08-24T06:12:39.648308Z","iopub.status.idle":"2023-08-24T06:12:39.735891Z","shell.execute_reply.started":"2023-08-24T06:12:39.648272Z","shell.execute_reply":"2023-08-24T06:12:39.734941Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Let's pass the training arguments to Trainer along with the model, dataset, tokenizer, data collator, and compute_metrics function:","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:39.739993Z","iopub.execute_input":"2023-08-24T06:12:39.740696Z","iopub.status.idle":"2023-08-24T06:12:45.792631Z","shell.execute_reply.started":"2023-08-24T06:12:39.740661Z","shell.execute_reply":"2023-08-24T06:12:45.791602Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Let me train the model:","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:12:45.793958Z","iopub.execute_input":"2023-08-24T06:12:45.794313Z","iopub.status.idle":"2023-08-24T06:16:17.731874Z","shell.execute_reply.started":"2023-08-24T06:12:45.794281Z","shell.execute_reply":"2023-08-24T06:16:17.730920Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1335' max='1335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1335/1335 03:25, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.353092</td>\n      <td>0.848968</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.351400</td>\n      <td>0.346445</td>\n      <td>0.851782</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.351400</td>\n      <td>0.406333</td>\n      <td>0.853659</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.149800</td>\n      <td>0.498571</td>\n      <td>0.848030</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.149800</td>\n      <td>0.517557</td>\n      <td>0.852720</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1335, training_loss=0.2091755884863464, metrics={'train_runtime': 211.5747, 'train_samples_per_second': 201.584, 'train_steps_per_second': 6.31, 'total_flos': 579351859980552.0, 'train_loss': 0.2091755884863464, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"Let's get a text:","metadata":{}},{"cell_type":"code","source":"text = 'That movie was great !'","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:21:56.887381Z","iopub.execute_input":"2023-08-24T06:21:56.888060Z","iopub.status.idle":"2023-08-24T06:21:56.896870Z","shell.execute_reply.started":"2023-08-24T06:21:56.888011Z","shell.execute_reply":"2023-08-24T06:21:56.894332Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Let's tokenize this text:","metadata":{}},{"cell_type":"code","source":"inputs = tokenizer(text,return_tensors=\"pt\")\ninputs","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:21:59.552815Z","iopub.execute_input":"2023-08-24T06:21:59.553246Z","iopub.status.idle":"2023-08-24T06:21:59.569000Z","shell.execute_reply.started":"2023-08-24T06:21:59.553212Z","shell.execute_reply":"2023-08-24T06:21:59.568000Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 101, 2008, 3185, 2001, 2307,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's load the model and predict the label:","metadata":{}},{"cell_type":"code","source":"import torch\nmodel_path = \"/kaggle/working/my_great_model/checkpoint-1068\"\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_path, num_labels=2)\n\nwith torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:22:42.307924Z","iopub.execute_input":"2023-08-24T06:22:42.308300Z","iopub.status.idle":"2023-08-24T06:22:43.176994Z","shell.execute_reply.started":"2023-08-24T06:22:42.308268Z","shell.execute_reply":"2023-08-24T06:22:43.175887Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the label of the prediction:","metadata":{}},{"cell_type":"code","source":"predicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:22:45.169969Z","iopub.execute_input":"2023-08-24T06:22:45.170314Z","iopub.status.idle":"2023-08-24T06:22:45.178019Z","shell.execute_reply.started":"2023-08-24T06:22:45.170283Z","shell.execute_reply":"2023-08-24T06:22:45.176984Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'POSITIVE'"},"metadata":{}}]}]}