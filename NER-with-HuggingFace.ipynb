{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tirendazacademy/ner-with-huggingface?scriptVersionId=149063399\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"![](https://assets-global.website-files.com/62a9e41d28a7ab25849bce9c/62fd1b91678fbb2e45fbd7f5_What-is-Named-Entity-Recognition-in-Natural-Language-Processing.jpg)","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we'll walk you through how to perform named entity recognition (NER) with Hugging Face using DistilBERT. If you want, you can watch our youtube video about NER [here](https://youtu.be/r-yR8-7dlvQ). \n\nLet's dive in!","metadata":{}},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Loading the Dataset </div></b>","metadata":{}},{"cell_type":"markdown","source":"The dataset we're going to use is [wnut_17](https://huggingface.co/datasets/wnut_17), which contains emerging and rare entities. Let's start with loading the dataset. ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nwnut = load_dataset(\"wnut_17\")\nwnut","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-02T17:56:24.212817Z","iopub.execute_input":"2023-11-02T17:56:24.213205Z","iopub.status.idle":"2023-11-02T17:56:29.070043Z","shell.execute_reply.started":"2023-11-02T17:56:24.213163Z","shell.execute_reply":"2023-11-02T17:56:29.069155Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a17243f0a07478182b6214b4641e2aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a84528536044cc1a10455da0527afbe"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wnut_17/wnut_17 (download: 782.18 KiB, generated: 1.66 MiB, post-processed: Unknown size, total: 2.43 MiB) to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6b0c3ea58c240478f5bb9b7e5a6ef56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8748629af83c40cc857a8e2775344be0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/39.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22d677221fcc4ca395f13a492be78b24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/66.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34fb4062c51046848033996a451c55a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a634698cb1454a3f93129475d701eafd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3394 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1287 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset wnut_17 downloaded and prepared to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c84b8335846b4b6e808210763eb78b1d"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 3394\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 1009\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 1287\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see, dataset consists of three dataset: Train, validation and test. Let's take a look at the first row of the train set.","metadata":{}},{"cell_type":"code","source":"print(wnut[\"train\"][0])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:56:29.071921Z","iopub.execute_input":"2023-11-02T17:56:29.072272Z","iopub.status.idle":"2023-11-02T17:56:29.079164Z","shell.execute_reply.started":"2023-11-02T17:56:29.072244Z","shell.execute_reply":"2023-11-02T17:56:29.078056Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The dataset includes three features: id, tokens and ner_tags. Let's examine the NER tags:","metadata":{}},{"cell_type":"code","source":"label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\nlabel_list","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:56:29.080253Z","iopub.execute_input":"2023-11-02T17:56:29.080539Z","iopub.status.idle":"2023-11-02T17:56:29.09192Z","shell.execute_reply.started":"2023-11-02T17:56:29.080516Z","shell.execute_reply":"2023-11-02T17:56:29.090961Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['O',\n 'B-corporation',\n 'I-corporation',\n 'B-creative-work',\n 'I-creative-work',\n 'B-group',\n 'I-group',\n 'B-location',\n 'I-location',\n 'B-person',\n 'I-person',\n 'B-product',\n 'I-product']"},"metadata":{}}]},{"cell_type":"markdown","source":"Awesome, we expored the dataset. Let's move on to preprocessing.\n\n# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Preprocessing </div></b>\n\nThe model we're going to use is a BERT-based model that is DistilBERT. Let's load the tokenizer of this model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:56:29.093044Z","iopub.execute_input":"2023-11-02T17:56:29.09332Z","iopub.status.idle":"2023-11-02T17:56:32.710869Z","shell.execute_reply.started":"2023-11-02T17:56:29.093297Z","shell.execute_reply":"2023-11-02T17:56:32.710019Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1848fa97b104ed3bf5580acd7929f71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"725df9976bd947409d701ada95b2eb12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3876076291d44bdfb9ad1e011dbba9b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"876cb60423ae4a7e94668898039277dc"}},"metadata":{}}]},{"cell_type":"markdown","source":"Nice we loaded our tokenizer. Let's try it on a text:","metadata":{}},{"cell_type":"code","source":"example = wnut[\"train\"][0]\ntokenized_input = tokenizer(example[\"tokens\"], is_split_into_words = True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:56:32.714499Z","iopub.execute_input":"2023-11-02T17:56:32.715055Z","iopub.status.idle":"2023-11-02T17:56:32.729981Z","shell.execute_reply.started":"2023-11-02T17:56:32.715027Z","shell.execute_reply":"2023-11-02T17:56:32.728976Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['[CLS]', '@', 'paul', '##walk', 'it', \"'\", 's', 'the', 'view', 'from', 'where', 'i', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see our text was tokenized and split into subwords. \n\nTo tokenize and align, let's create a function. This preprocessing is necessary for NER.","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples[f\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:56:32.73129Z","iopub.execute_input":"2023-11-02T17:56:32.731703Z","iopub.status.idle":"2023-11-02T17:56:32.739183Z","shell.execute_reply.started":"2023-11-02T17:56:32.731649Z","shell.execute_reply":"2023-11-02T17:56:32.738441Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Excellent, let's tokenize the entire dataset using this function:","metadata":{}},{"cell_type":"code","source":"tokenized_wnut = wnut.map( tokenize_and_align_labels, batched = True)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:56:32.740198Z","iopub.execute_input":"2023-11-02T17:56:32.74045Z","iopub.status.idle":"2023-11-02T17:56:33.739921Z","shell.execute_reply.started":"2023-11-02T17:56:32.740428Z","shell.execute_reply":"2023-11-02T17:56:33.739038Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67e445aedf26465b89d12b11e40da935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2504576da2874280ae25e1e11ea71f07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2531bad2bb584bec9f449024d6748d07"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's move on to padding. To do this, we're going to use data collator.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\ndata_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:56:33.741084Z","iopub.execute_input":"2023-11-02T17:56:33.741355Z","iopub.status.idle":"2023-11-02T17:56:50.636435Z","shell.execute_reply.started":"2023-11-02T17:56:33.741331Z","shell.execute_reply":"2023-11-02T17:56:50.635545Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's move on to setting the metrics. To do this, we're going to use the evaluate and seqeval libraries. First, let's load these libraries.","metadata":{}},{"cell_type":"code","source":"!pip install -q evaluate seqeval","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:56:50.637641Z","iopub.execute_input":"2023-11-02T17:56:50.638345Z","iopub.status.idle":"2023-11-02T17:57:08.232382Z","shell.execute_reply.started":"2023-11-02T17:56:50.638308Z","shell.execute_reply":"2023-11-02T17:57:08.231039Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's import these libraries.","metadata":{}},{"cell_type":"code","source":"import evaluate\nseqeval = evaluate.load(\"seqeval\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:57:08.234081Z","iopub.execute_input":"2023-11-02T17:57:08.234485Z","iopub.status.idle":"2023-11-02T17:57:12.373069Z","shell.execute_reply.started":"2023-11-02T17:57:08.234438Z","shell.execute_reply":"2023-11-02T17:57:12.372172Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"942e2389dbcb4e18a7a999b722ee8f39"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now, let me create a function to calculate the metrics.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nlabels = [label_list[i] for i in example[f\"ner_tags\"]]\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:57:12.374178Z","iopub.execute_input":"2023-11-02T17:57:12.37447Z","iopub.status.idle":"2023-11-02T17:57:12.382339Z","shell.execute_reply.started":"2023-11-02T17:57:12.374443Z","shell.execute_reply":"2023-11-02T17:57:12.381382Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Model Training </div></b>","metadata":{}},{"cell_type":"markdown","source":"In this section, we're going to train the BERT-based model. First, let's create two variables that map labels and IDs.","metadata":{}},{"cell_type":"code","source":"id2label = {\n    0: \"O\",\n    1: \"B-corporation\",\n    2: \"I-corporation\",\n    3: \"B-creative-work\",\n    4: \"I-creative-work\",\n    5: \"B-group\",\n    6: \"I-group\",\n    7: \"B-location\",\n    8: \"I-location\",\n    9: \"B-person\",\n    10: \"I-person\",\n    11: \"B-product\",\n    12: \"I-product\",\n    }\nlabel2id = {\n    \"O\": 0,\n    \"B-corporation\": 1,\n    \"I-corporation\": 2,\n    \"B-creative-work\": 3,\n    \"I-creative-work\": 4,\n    \"B-group\": 5,\n    \"I-group\": 6,\n    \"B-location\": 7,\n    \"I-location\": 8,\n    \"B-person\": 9,\n    \"I-person\": 10,\n    \"B-product\": 11,\n    \"I-product\": 12,\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:57:12.38368Z","iopub.execute_input":"2023-11-02T17:57:12.384325Z","iopub.status.idle":"2023-11-02T17:57:12.401681Z","shell.execute_reply.started":"2023-11-02T17:57:12.384291Z","shell.execute_reply":"2023-11-02T17:57:12.400934Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"It's time to load the DistilBERT model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels = 13,\n    id2label = id2label,\n    label2id= label2id\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:57:12.403087Z","iopub.execute_input":"2023-11-02T17:57:12.403363Z","iopub.status.idle":"2023-11-02T17:57:15.139055Z","shell.execute_reply.started":"2023-11-02T17:57:12.40334Z","shell.execute_reply":"2023-11-02T17:57:15.13826Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2caccb0fa5aa43be8d7ddad0a4bd6604"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To load the model the Hub, let's login it.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:57:15.142557Z","iopub.execute_input":"2023-11-02T17:57:15.142889Z","iopub.status.idle":"2023-11-02T17:57:15.171036Z","shell.execute_reply.started":"2023-11-02T17:57:15.142864Z","shell.execute_reply":"2023-11-02T17:57:15.170186Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1c0d8e9c4a947688499bf19c8768907"}},"metadata":{}}]},{"cell_type":"markdown","source":"To train, let's set hyperparameters using the TrainingArguments class.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir = \"my_ner_model\",\n    learning_rate = 2e-5,\n    per_device_train_batch_size = 16,\n    per_device_eval_batch_size=16,\n    num_train_epochs = 2,\n    weight_decay = 0.01,\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    load_best_model_at_end = True,\n    report_to = [\"none\"],\n    push_to_hub = True, \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:58:03.990626Z","iopub.execute_input":"2023-11-02T17:58:03.990993Z","iopub.status.idle":"2023-11-02T17:58:04.068676Z","shell.execute_reply.started":"2023-11-02T17:58:03.990963Z","shell.execute_reply":"2023-11-02T17:58:04.067653Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"After that, let's instantiate a Trainer object.","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model = model,\n    train_dataset = tokenized_wnut[\"train\"],\n    eval_dataset = tokenized_wnut[\"test\"],\n    tokenizer = tokenizer,\n    data_collator = data_collator,\n    compute_metrics = compute_metrics,\n    args = training_args\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:58:04.070774Z","iopub.execute_input":"2023-11-02T17:58:04.071121Z","iopub.status.idle":"2023-11-02T17:58:12.022353Z","shell.execute_reply.started":"2023-11-02T17:58:04.07109Z","shell.execute_reply":"2023-11-02T17:58:12.021509Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The model is ready to fine tune. Let's call the train method to train.","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:58:12.023527Z","iopub.execute_input":"2023-11-02T17:58:12.023841Z","iopub.status.idle":"2023-11-02T17:59:16.421209Z","shell.execute_reply.started":"2023-11-02T17:58:12.023815Z","shell.execute_reply":"2023-11-02T17:59:16.420255Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='214' max='214' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [214/214 00:43, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.298523</td>\n      <td>0.383562</td>\n      <td>0.155700</td>\n      <td>0.221490</td>\n      <td>0.933179</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.288267</td>\n      <td>0.458078</td>\n      <td>0.207600</td>\n      <td>0.285714</td>\n      <td>0.936600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=214, training_loss=0.2673697605311314, metrics={'train_runtime': 51.2977, 'train_samples_per_second': 132.326, 'train_steps_per_second': 4.172, 'total_flos': 98230856448120.0, 'train_loss': 0.2673697605311314, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Awesome, training is completed. Let's push it to the Hub.","metadata":{}},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:59:16.422351Z","iopub.execute_input":"2023-11-02T17:59:16.422669Z","iopub.status.idle":"2023-11-02T17:59:19.24869Z","shell.execute_reply.started":"2023-11-02T17:59:16.422642Z","shell.execute_reply":"2023-11-02T17:59:19.247655Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Tirendaz/my_ner_model/tree/main/'"},"metadata":{}}]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Prediction </div></b>","metadata":{}},{"cell_type":"markdown","source":"To inference, let's get a text and classifier its tokens.","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\ntext = \"My name is Sarah, I live in London\"\nclassifier = pipeline(\"ner\", model=\"Tirendaz/my_ner_model\")\nclassifier(text)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:59:19.250612Z","iopub.execute_input":"2023-11-02T17:59:19.250908Z","iopub.status.idle":"2023-11-02T17:59:24.141311Z","shell.execute_reply.started":"2023-11-02T17:59:19.250882Z","shell.execute_reply":"2023-11-02T17:59:24.1405Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca85de527eb4e98a08ce8251d3c1195"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/266M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8695c6ea24a94b4f814e27fec72313e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0110bb37ac74bad99008ea79f932a8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11731edf4f1e43b482b6bc673f05fa35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef1a4613d404feeb017e73c9b2fd9ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fce77a2febee49b4945674c67f048071"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[{'entity': 'B-person',\n  'score': 0.53155184,\n  'index': 4,\n  'word': 'sarah',\n  'start': 11,\n  'end': 16},\n {'entity': 'B-location',\n  'score': 0.3932454,\n  'index': 9,\n  'word': 'london',\n  'start': 28,\n  'end': 34}]"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see, our model works well. To see predicted labels, let's create a function.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef tag_sentence(text:str):\n    # convert our text to a  tokenized sequence\n    inputs = tokenizer(text, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n    # get outputs\n    outputs = model(**inputs)\n    # convert to probabilities with softmax\n    probs = outputs[0][0].softmax(1)\n    # get the tags with the highest probability\n    word_tags = [(tokenizer.decode(inputs['input_ids'][0][i].item()), id2label[tagid.item()]) \n                  for i, tagid in enumerate (probs.argmax(axis=1))]\n\n    return pd.DataFrame(word_tags, columns=['word', 'tag'])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:59:24.142767Z","iopub.execute_input":"2023-11-02T17:59:24.14313Z","iopub.status.idle":"2023-11-02T17:59:24.150143Z","shell.execute_reply.started":"2023-11-02T17:59:24.143096Z","shell.execute_reply":"2023-11-02T17:59:24.149104Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Now, let's predict tags in text.","metadata":{}},{"cell_type":"code","source":"print(tag_sentence(text))","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:59:24.151521Z","iopub.execute_input":"2023-11-02T17:59:24.151824Z","iopub.status.idle":"2023-11-02T17:59:25.919307Z","shell.execute_reply.started":"2023-11-02T17:59:24.151799Z","shell.execute_reply":"2023-11-02T17:59:25.918158Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"      word         tag\n0    [CLS]           O\n1       my           O\n2     name           O\n3       is           O\n4    sarah    B-person\n5        ,           O\n6        i           O\n7     live           O\n8       in           O\n9   london  B-location\n10   [SEP]           O\n","output_type":"stream"}]},{"cell_type":"markdown","source":"That's it. Thanks for reading. If you enjoy, don't forget to upvote â˜ºï¸\n\nLet's connect [YouTube](http://youtube.com/tirendazacademy) | [Medium](http://tirendazacademy.medium.com) | [X](http://x.com/tirendazacademy) | [Linkedin](https://www.linkedin.com/in/tirendaz-academy)\n\nHappy coding ðŸ˜€","metadata":{}}]}