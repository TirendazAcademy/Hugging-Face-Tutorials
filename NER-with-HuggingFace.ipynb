{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://assets-global.website-files.com/62a9e41d28a7ab25849bce9c/62fd1b91678fbb2e45fbd7f5_What-is-Named-Entity-Recognition-in-Natural-Language-Processing.jpg)","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we'll walk you through how to perform named entity recognition (NER) with Hugging Face using DistilBERT. If you want, you can watch our youtube video about NER below [here](https://youtu.be/r-yR8-7dlvQ). \n\nLet's dive in!","metadata":{}},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Loading the Dataset </div></b>","metadata":{}},{"cell_type":"markdown","source":"The dataset we're going to use is [wnut_17](https://huggingface.co/datasets/wnut_17), which contains emerging and rare entities. Let's start with loading the dataset. ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nwnut = load_dataset(\"wnut_17\")\nwnut","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-02T12:31:31.425856Z","iopub.execute_input":"2023-11-02T12:31:31.426112Z","iopub.status.idle":"2023-11-02T12:31:35.701792Z","shell.execute_reply.started":"2023-11-02T12:31:31.426087Z","shell.execute_reply":"2023-11-02T12:31:35.700373Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6596b3dd4c437fa869a0d9877e0a9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c385f5b2a77340359c60106a5007b0af"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wnut_17/wnut_17 (download: 782.18 KiB, generated: 1.66 MiB, post-processed: Unknown size, total: 2.43 MiB) to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc372f945f845509b8356beb243d9db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ebe47f7289443ca358870b1cb68b79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/39.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40b79fceb63d43d58e6fe427b66795af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/66.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"180b10d9b10b4039b5501834eed504f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd1a6fe53f4408c98bc949132577b8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3394 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1287 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset wnut_17 downloaded and prepared to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c311d475f954a53bafa0d75c1704000"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 3394\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 1009\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 1287\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see, dataset consists of three dataset: Train, validation and test. Let's take a look at the first row of the train set.","metadata":{}},{"cell_type":"code","source":"print(wnut[\"train\"][0])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:31:35.704441Z","iopub.execute_input":"2023-11-02T12:31:35.705109Z","iopub.status.idle":"2023-11-02T12:31:35.711656Z","shell.execute_reply.started":"2023-11-02T12:31:35.705072Z","shell.execute_reply":"2023-11-02T12:31:35.710686Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The dataset includes three features: id, tokens and ner_tags. Let's examine the NER tags:","metadata":{}},{"cell_type":"code","source":"label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\nlabel_list","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:31:35.713049Z","iopub.execute_input":"2023-11-02T12:31:35.713615Z","iopub.status.idle":"2023-11-02T12:31:35.732688Z","shell.execute_reply.started":"2023-11-02T12:31:35.713580Z","shell.execute_reply":"2023-11-02T12:31:35.731837Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['O',\n 'B-corporation',\n 'I-corporation',\n 'B-creative-work',\n 'I-creative-work',\n 'B-group',\n 'I-group',\n 'B-location',\n 'I-location',\n 'B-person',\n 'I-person',\n 'B-product',\n 'I-product']"},"metadata":{}}]},{"cell_type":"markdown","source":"Awesome, we expored the dataset. Let's move on to preprocessing.\n\n# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Preprocessing </div></b>\n\nThe model we're going to use is a BERT-based model that is DistilBERT. Let's load the tokenizer of this model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:31:35.735294Z","iopub.execute_input":"2023-11-02T12:31:35.735636Z","iopub.status.idle":"2023-11-02T12:31:38.203156Z","shell.execute_reply.started":"2023-11-02T12:31:35.735605Z","shell.execute_reply":"2023-11-02T12:31:38.202225Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72b335d30fed46108d77835d05ae7e46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"974d3d95611b45aab73d5f550ca15ac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c8018cb12a43b8b682b19860b2545d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5487dc4f47114e02ba024263859f166d"}},"metadata":{}}]},{"cell_type":"markdown","source":"Nice we loaded our tokenizer. Let's try it on a text:","metadata":{}},{"cell_type":"code","source":"example = wnut[\"train\"][0]\ntokenized_input = tokenizer(example[\"tokens\"], is_split_into_words = True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:31:38.204282Z","iopub.execute_input":"2023-11-02T12:31:38.204710Z","iopub.status.idle":"2023-11-02T12:31:38.214714Z","shell.execute_reply.started":"2023-11-02T12:31:38.204683Z","shell.execute_reply":"2023-11-02T12:31:38.213678Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['[CLS]', '@', 'paul', '##walk', 'it', \"'\", 's', 'the', 'view', 'from', 'where', 'i', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see our text was tokenized and split into subwords. \n\nTo tokenize and align, let's create a function. This preprocessing is necessary for NER.","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples[f\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:31:38.215850Z","iopub.execute_input":"2023-11-02T12:31:38.216163Z","iopub.status.idle":"2023-11-02T12:31:38.223891Z","shell.execute_reply.started":"2023-11-02T12:31:38.216138Z","shell.execute_reply":"2023-11-02T12:31:38.223035Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Excellent, let's tokenize the entire dataset using this function:","metadata":{}},{"cell_type":"code","source":"tokenized_wnut = wnut.map( tokenize_and_align_labels, batched = True)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:31:38.225043Z","iopub.execute_input":"2023-11-02T12:31:38.225321Z","iopub.status.idle":"2023-11-02T12:31:39.181012Z","shell.execute_reply.started":"2023-11-02T12:31:38.225295Z","shell.execute_reply":"2023-11-02T12:31:39.180068Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"577895472ceb4a25929f0d480983eb6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ade2b86488ac4866b3f722f4c8e97c36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b8c7db7b23b404a80c533e8b3084a90"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's move on to padding. To do this, we're going to use data collator.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\ndata_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:31:39.182058Z","iopub.execute_input":"2023-11-02T12:31:39.182335Z","iopub.status.idle":"2023-11-02T12:31:49.295724Z","shell.execute_reply.started":"2023-11-02T12:31:39.182310Z","shell.execute_reply":"2023-11-02T12:31:49.294932Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's move on to setting the metrics. To do this, we're going to use the evaluate and seqeval libraries. First, let's load these libraries.","metadata":{}},{"cell_type":"code","source":"!pip install -q evaluate seqeval","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:31:49.296789Z","iopub.execute_input":"2023-11-02T12:31:49.297342Z","iopub.status.idle":"2023-11-02T12:32:05.377114Z","shell.execute_reply.started":"2023-11-02T12:31:49.297314Z","shell.execute_reply":"2023-11-02T12:32:05.376078Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's import these libraries.","metadata":{}},{"cell_type":"code","source":"import evaluate\nseqeval = evaluate.load(\"seqeval\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:32:05.381379Z","iopub.execute_input":"2023-11-02T12:32:05.381705Z","iopub.status.idle":"2023-11-02T12:32:08.769432Z","shell.execute_reply.started":"2023-11-02T12:32:05.381675Z","shell.execute_reply":"2023-11-02T12:32:08.768519Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42f26d3599f543f6ba1a129e30d434ef"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now, let me create a function to calculate the metrics.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nlabels = [label_list[i] for i in example[f\"ner_tags\"]]\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:32:08.770629Z","iopub.execute_input":"2023-11-02T12:32:08.770925Z","iopub.status.idle":"2023-11-02T12:32:08.778633Z","shell.execute_reply.started":"2023-11-02T12:32:08.770898Z","shell.execute_reply":"2023-11-02T12:32:08.777601Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Model Training </div></b>","metadata":{}},{"cell_type":"markdown","source":"In this section, we're going to train the BERT-based model. First, let's create two variables that map labels and IDs.","metadata":{}},{"cell_type":"code","source":"id2label = {\n    0: \"O\",\n    1: \"B-corporation\",\n    2: \"I-corporation\",\n    3: \"B-creative-work\",\n    4: \"I-creative-work\",\n    5: \"B-group\",\n    6: \"I-group\",\n    7: \"B-location\",\n    8: \"I-location\",\n    9: \"B-person\",\n    10: \"I-person\",\n    11: \"B-product\",\n    12: \"I-product\",\n    }\nlabel2id = {\n    \"O\": 0,\n    \"B-corporation\": 1,\n    \"I-corporation\": 2,\n    \"B-creative-work\": 3,\n    \"I-creative-work\": 4,\n    \"B-group\": 5,\n    \"I-group\": 6,\n    \"B-location\": 7,\n    \"I-location\": 8,\n    \"B-person\": 9,\n    \"I-person\": 10,\n    \"B-product\": 11,\n    \"I-product\": 12,\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:32:08.780663Z","iopub.execute_input":"2023-11-02T12:32:08.781415Z","iopub.status.idle":"2023-11-02T12:32:08.798149Z","shell.execute_reply.started":"2023-11-02T12:32:08.781314Z","shell.execute_reply":"2023-11-02T12:32:08.797280Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"It's time to load the DistilBERT model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels = 13,\n    id2label = id2label,\n    label2id= label2id\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:32:08.799298Z","iopub.execute_input":"2023-11-02T12:32:08.799568Z","iopub.status.idle":"2023-11-02T12:32:11.668477Z","shell.execute_reply.started":"2023-11-02T12:32:08.799513Z","shell.execute_reply":"2023-11-02T12:32:11.667704Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5395a5d71c964929b7fd1dbd7f5cf31a"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To load the model the Hub, let's login it.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:32:23.951158Z","iopub.execute_input":"2023-11-02T12:32:23.951558Z","iopub.status.idle":"2023-11-02T12:32:23.979914Z","shell.execute_reply.started":"2023-11-02T12:32:23.951523Z","shell.execute_reply":"2023-11-02T12:32:23.978996Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cbdbc73bff946ba992207efb4679410"}},"metadata":{}}]},{"cell_type":"markdown","source":"To train, let's set hyperparameters using the TrainingArguments class.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir = \"my_ner_model\",\n    learning_rate = 2e-5,\n    per_device_train_batch_size = 16,\n    per_device_eval_batch_size=16,\n    num_train_epochs = 2,\n    weight_decay = 0.01,\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    load_best_model_at_end = True,\n    report_to = [\"none\"],\n    push_to_hub = True, \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:32:33.350470Z","iopub.execute_input":"2023-11-02T12:32:33.351010Z","iopub.status.idle":"2023-11-02T12:32:33.435406Z","shell.execute_reply.started":"2023-11-02T12:32:33.350973Z","shell.execute_reply":"2023-11-02T12:32:33.433955Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"After that, let's instantiate a Trainer object.","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model = model,\n    train_dataset = tokenized_wnut[\"train\"],\n    eval_dataset = tokenized_wnut[\"test\"],\n    tokenizer = tokenizer,\n    data_collator = data_collator,\n    compute_metrics = compute_metrics,\n    args = training_args\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:32:33.437620Z","iopub.execute_input":"2023-11-02T12:32:33.437997Z","iopub.status.idle":"2023-11-02T12:32:39.999139Z","shell.execute_reply.started":"2023-11-02T12:32:33.437964Z","shell.execute_reply":"2023-11-02T12:32:39.998364Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The model is ready to fine tune. Let's call the train method to train.","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:32:40.000255Z","iopub.execute_input":"2023-11-02T12:32:40.000542Z","iopub.status.idle":"2023-11-02T12:33:36.958054Z","shell.execute_reply.started":"2023-11-02T12:32:40.000517Z","shell.execute_reply":"2023-11-02T12:33:36.957026Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='214' max='214' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [214/214 00:41, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.329615</td>\n      <td>0.301587</td>\n      <td>0.035218</td>\n      <td>0.063071</td>\n      <td>0.928349</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.312901</td>\n      <td>0.377778</td>\n      <td>0.157553</td>\n      <td>0.222368</td>\n      <td>0.933863</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=214, training_loss=0.2968178330180801, metrics={'train_runtime': 47.1529, 'train_samples_per_second': 143.957, 'train_steps_per_second': 4.538, 'total_flos': 98230856448120.0, 'train_loss': 0.2968178330180801, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Awesome, training is completed. Let's push it to the Hub.","metadata":{}},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:33:36.960940Z","iopub.execute_input":"2023-11-02T12:33:36.961591Z","iopub.status.idle":"2023-11-02T12:33:39.555217Z","shell.execute_reply.started":"2023-11-02T12:33:36.961558Z","shell.execute_reply":"2023-11-02T12:33:39.554178Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Tirendaz/my_ner_model/tree/main/'"},"metadata":{}}]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Prediction </div></b>","metadata":{}},{"cell_type":"markdown","source":"To inference, let's get a text and classifier its tokens.","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\ntext = \"My name is Sarah, I live in London\"\nclassifier = pipeline(\"ner\", model=\"Tirendaz/my_ner_model\")\nclassifier(text)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:33:39.556216Z","iopub.execute_input":"2023-11-02T12:33:39.556479Z","iopub.status.idle":"2023-11-02T12:33:44.543540Z","shell.execute_reply.started":"2023-11-02T12:33:39.556455Z","shell.execute_reply":"2023-11-02T12:33:44.542424Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbf77456fbaa47999ca53d33aaf334db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/266M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a01eed6fedaf4d6b87229384891a0c9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dc42c49d52e4cb694620c75fccfb52d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90566e78850146eda04300ce3b071186"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eaa1344bcf04bffb3743ba28f0daecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c47628d85b14538bc31200572cfdc29"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see, our model works well. To see predicted labels, let's create a function.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef tag_sentence(text:str):\n    # convert our text to a  tokenized sequence\n    inputs = tokenizer(text, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n    # get outputs\n    outputs = model(**inputs)\n    # convert to probabilities with softmax\n    probs = outputs[0][0].softmax(1)\n    # get the tags with the highest probability\n    word_tags = [(tokenizer.decode(inputs['input_ids'][0][i].item()), id2label[tagid.item()]) \n                  for i, tagid in enumerate (probs.argmax(axis=1))]\n\n    return pd.DataFrame(word_tags, columns=['word', 'tag'])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:33:44.544724Z","iopub.execute_input":"2023-11-02T12:33:44.545053Z","iopub.status.idle":"2023-11-02T12:33:44.554695Z","shell.execute_reply.started":"2023-11-02T12:33:44.545025Z","shell.execute_reply":"2023-11-02T12:33:44.553826Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Now, let's predict tags in text.","metadata":{}},{"cell_type":"code","source":"print(tag_sentence(text))","metadata":{"execution":{"iopub.status.busy":"2023-11-02T12:33:44.556096Z","iopub.execute_input":"2023-11-02T12:33:44.556476Z","iopub.status.idle":"2023-11-02T12:33:46.201660Z","shell.execute_reply.started":"2023-11-02T12:33:44.556442Z","shell.execute_reply":"2023-11-02T12:33:46.200782Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"      word tag\n0    [CLS]   O\n1       my   O\n2     name   O\n3       is   O\n4    sarah   O\n5        ,   O\n6        i   O\n7     live   O\n8       in   O\n9   london   O\n10   [SEP]   O\n","output_type":"stream"}]},{"cell_type":"markdown","source":"That's it. Thanks for reading. If you enjoy, don't forget to upvote â˜ºï¸\n\nLet's connect [YouTube](http://youtube.com/tirendazacademy) | [Medium](http://tirendazacademy.medium.com) | [X](http://x.com/tirendazacademy).\n\nHappy coding ðŸ˜€","metadata":{}}]}