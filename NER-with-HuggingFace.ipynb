{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://assets-global.website-files.com/62a9e41d28a7ab25849bce9c/62fd1b91678fbb2e45fbd7f5_What-is-Named-Entity-Recognition-in-Natural-Language-Processing.jpg)","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we'll walk you through how to perform named entity recognition (NER) with Hugging Face using DistilBERT. If you want, you can watch our youtube video about NER below [here](https://youtu.be/r-yR8-7dlvQ). \n\nLet's dive in!","metadata":{}},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Loading the Dataset </div></b>","metadata":{}},{"cell_type":"markdown","source":"The dataset we're going to use is [wnut_17](https://huggingface.co/datasets/wnut_17), which contains emerging and rare entities. Let's start with loading the dataset. ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nwnut = load_dataset(\"wnut_17\")\nwnut","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-02T13:04:33.394581Z","iopub.execute_input":"2023-11-02T13:04:33.395008Z","iopub.status.idle":"2023-11-02T13:04:37.910978Z","shell.execute_reply.started":"2023-11-02T13:04:33.394975Z","shell.execute_reply":"2023-11-02T13:04:37.910057Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528e0ce01a92482c8cc802af72a15142"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbf945eb40a645e192e187989b3da2f4"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wnut_17/wnut_17 (download: 782.18 KiB, generated: 1.66 MiB, post-processed: Unknown size, total: 2.43 MiB) to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"792d149f7ea943e8ae6a1e5a06a998d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"405b17469f2d47ebad4b70de312410aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/39.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d02f8e826b1743038de32ca90e38c3f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/66.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6fddd91e4149bbab512233a1a01f20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0636b676a7634ffb9e08502a7ade4e72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3394 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1287 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset wnut_17 downloaded and prepared to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66943bacd08c4b599b59cd87cb8e7c50"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 3394\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 1009\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 1287\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see, dataset consists of three dataset: Train, validation and test. Let's take a look at the first row of the train set.","metadata":{}},{"cell_type":"code","source":"print(wnut[\"train\"][0])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:04:37.913129Z","iopub.execute_input":"2023-11-02T13:04:37.913879Z","iopub.status.idle":"2023-11-02T13:04:37.921073Z","shell.execute_reply.started":"2023-11-02T13:04:37.913842Z","shell.execute_reply":"2023-11-02T13:04:37.920093Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The dataset includes three features: id, tokens and ner_tags. Let's examine the NER tags:","metadata":{}},{"cell_type":"code","source":"label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\nlabel_list","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:04:37.922216Z","iopub.execute_input":"2023-11-02T13:04:37.922492Z","iopub.status.idle":"2023-11-02T13:04:37.938928Z","shell.execute_reply.started":"2023-11-02T13:04:37.922468Z","shell.execute_reply":"2023-11-02T13:04:37.937920Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['O',\n 'B-corporation',\n 'I-corporation',\n 'B-creative-work',\n 'I-creative-work',\n 'B-group',\n 'I-group',\n 'B-location',\n 'I-location',\n 'B-person',\n 'I-person',\n 'B-product',\n 'I-product']"},"metadata":{}}]},{"cell_type":"markdown","source":"Awesome, we expored the dataset. Let's move on to preprocessing.\n\n# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Preprocessing </div></b>\n\nThe model we're going to use is a BERT-based model that is DistilBERT. Let's load the tokenizer of this model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:04:37.940268Z","iopub.execute_input":"2023-11-02T13:04:37.940992Z","iopub.status.idle":"2023-11-02T13:04:41.498426Z","shell.execute_reply.started":"2023-11-02T13:04:37.940965Z","shell.execute_reply":"2023-11-02T13:04:41.497583Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b53e359ca2084e5d995f46f56c57eb96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ec9ca886c054c059fa7b10438da937d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e08c54967ec94e7ca571599eaf3c38ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd01fc6ffa3c4737bb2cb24f400be030"}},"metadata":{}}]},{"cell_type":"markdown","source":"Nice we loaded our tokenizer. Let's try it on a text:","metadata":{}},{"cell_type":"code","source":"example = wnut[\"train\"][0]\ntokenized_input = tokenizer(example[\"tokens\"], is_split_into_words = True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:04:41.501496Z","iopub.execute_input":"2023-11-02T13:04:41.502029Z","iopub.status.idle":"2023-11-02T13:04:41.517460Z","shell.execute_reply.started":"2023-11-02T13:04:41.501999Z","shell.execute_reply":"2023-11-02T13:04:41.516507Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['[CLS]', '@', 'paul', '##walk', 'it', \"'\", 's', 'the', 'view', 'from', 'where', 'i', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see our text was tokenized and split into subwords. \n\nTo tokenize and align, let's create a function. This preprocessing is necessary for NER.","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples[f\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:04:41.518616Z","iopub.execute_input":"2023-11-02T13:04:41.518944Z","iopub.status.idle":"2023-11-02T13:04:41.526172Z","shell.execute_reply.started":"2023-11-02T13:04:41.518917Z","shell.execute_reply":"2023-11-02T13:04:41.525289Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Excellent, let's tokenize the entire dataset using this function:","metadata":{}},{"cell_type":"code","source":"tokenized_wnut = wnut.map( tokenize_and_align_labels, batched = True)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:04:41.528304Z","iopub.execute_input":"2023-11-02T13:04:41.528601Z","iopub.status.idle":"2023-11-02T13:04:42.506989Z","shell.execute_reply.started":"2023-11-02T13:04:41.528576Z","shell.execute_reply":"2023-11-02T13:04:42.506092Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60c45b5e32dd4ec2aa0bac5c8e495a7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1bccd6015e4e0a854a15f4dc1aa5d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b9e26362b24b968eadaebaf48e65eb"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's move on to padding. To do this, we're going to use data collator.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\ndata_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:04:42.508234Z","iopub.execute_input":"2023-11-02T13:04:42.508615Z","iopub.status.idle":"2023-11-02T13:04:59.121548Z","shell.execute_reply.started":"2023-11-02T13:04:42.508574Z","shell.execute_reply":"2023-11-02T13:04:59.120677Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's move on to setting the metrics. To do this, we're going to use the evaluate and seqeval libraries. First, let's load these libraries.","metadata":{}},{"cell_type":"code","source":"!pip install -q evaluate seqeval","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:04:59.122764Z","iopub.execute_input":"2023-11-02T13:04:59.123357Z","iopub.status.idle":"2023-11-02T13:05:16.977213Z","shell.execute_reply.started":"2023-11-02T13:04:59.123328Z","shell.execute_reply":"2023-11-02T13:05:16.976027Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's import these libraries.","metadata":{}},{"cell_type":"code","source":"import evaluate\nseqeval = evaluate.load(\"seqeval\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:05:16.978925Z","iopub.execute_input":"2023-11-02T13:05:16.979243Z","iopub.status.idle":"2023-11-02T13:05:21.071932Z","shell.execute_reply.started":"2023-11-02T13:05:16.979216Z","shell.execute_reply":"2023-11-02T13:05:21.071027Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b72290b38fd47619b19b1181deb45fd"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now, let me create a function to calculate the metrics.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nlabels = [label_list[i] for i in example[f\"ner_tags\"]]\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:05:21.073045Z","iopub.execute_input":"2023-11-02T13:05:21.073327Z","iopub.status.idle":"2023-11-02T13:05:21.081061Z","shell.execute_reply.started":"2023-11-02T13:05:21.073302Z","shell.execute_reply":"2023-11-02T13:05:21.080139Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Model Training </div></b>","metadata":{}},{"cell_type":"markdown","source":"In this section, we're going to train the BERT-based model. First, let's create two variables that map labels and IDs.","metadata":{}},{"cell_type":"code","source":"id2label = {\n    0: \"O\",\n    1: \"B-corporation\",\n    2: \"I-corporation\",\n    3: \"B-creative-work\",\n    4: \"I-creative-work\",\n    5: \"B-group\",\n    6: \"I-group\",\n    7: \"B-location\",\n    8: \"I-location\",\n    9: \"B-person\",\n    10: \"I-person\",\n    11: \"B-product\",\n    12: \"I-product\",\n    }\nlabel2id = {\n    \"O\": 0,\n    \"B-corporation\": 1,\n    \"I-corporation\": 2,\n    \"B-creative-work\": 3,\n    \"I-creative-work\": 4,\n    \"B-group\": 5,\n    \"I-group\": 6,\n    \"B-location\": 7,\n    \"I-location\": 8,\n    \"B-person\": 9,\n    \"I-person\": 10,\n    \"B-product\": 11,\n    \"I-product\": 12,\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:05:21.082484Z","iopub.execute_input":"2023-11-02T13:05:21.082875Z","iopub.status.idle":"2023-11-02T13:05:21.101572Z","shell.execute_reply.started":"2023-11-02T13:05:21.082840Z","shell.execute_reply":"2023-11-02T13:05:21.100835Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"It's time to load the DistilBERT model:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels = 13,\n    id2label = id2label,\n    label2id= label2id\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:05:21.102740Z","iopub.execute_input":"2023-11-02T13:05:21.103081Z","iopub.status.idle":"2023-11-02T13:05:23.685130Z","shell.execute_reply.started":"2023-11-02T13:05:21.103048Z","shell.execute_reply":"2023-11-02T13:05:23.684351Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8278c1f4b0514c43aeba77d55fca11c0"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To load the model the Hub, let's login it.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:05:23.688571Z","iopub.execute_input":"2023-11-02T13:05:23.688953Z","iopub.status.idle":"2023-11-02T13:05:23.719599Z","shell.execute_reply.started":"2023-11-02T13:05:23.688926Z","shell.execute_reply":"2023-11-02T13:05:23.718721Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa74779a35fb42e2a8402d36524e613d"}},"metadata":{}}]},{"cell_type":"markdown","source":"To train, let's set hyperparameters using the TrainingArguments class.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir = \"my_ner_model\",\n    learning_rate = 2e-5,\n    per_device_train_batch_size = 16,\n    per_device_eval_batch_size=16,\n    num_train_epochs = 2,\n    weight_decay = 0.01,\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    load_best_model_at_end = True,\n    report_to = [\"none\"],\n    push_to_hub = True, \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:05:34.626950Z","iopub.execute_input":"2023-11-02T13:05:34.627318Z","iopub.status.idle":"2023-11-02T13:05:34.704463Z","shell.execute_reply.started":"2023-11-02T13:05:34.627289Z","shell.execute_reply":"2023-11-02T13:05:34.703427Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"After that, let's instantiate a Trainer object.","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model = model,\n    train_dataset = tokenized_wnut[\"train\"],\n    eval_dataset = tokenized_wnut[\"test\"],\n    tokenizer = tokenizer,\n    data_collator = data_collator,\n    compute_metrics = compute_metrics,\n    args = training_args\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:05:34.706189Z","iopub.execute_input":"2023-11-02T13:05:34.706499Z","iopub.status.idle":"2023-11-02T13:05:43.432002Z","shell.execute_reply.started":"2023-11-02T13:05:34.706473Z","shell.execute_reply":"2023-11-02T13:05:43.430981Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The model is ready to fine tune. Let's call the train method to train.","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:05:43.433317Z","iopub.execute_input":"2023-11-02T13:05:43.434016Z","iopub.status.idle":"2023-11-02T13:06:43.217688Z","shell.execute_reply.started":"2023-11-02T13:05:43.433979Z","shell.execute_reply":"2023-11-02T13:06:43.216755Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='214' max='214' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [214/214 00:41, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.301647</td>\n      <td>0.478261</td>\n      <td>0.132530</td>\n      <td>0.207547</td>\n      <td>0.933137</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.291263</td>\n      <td>0.522449</td>\n      <td>0.237257</td>\n      <td>0.326322</td>\n      <td>0.937925</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=214, training_loss=0.26252145856340353, metrics={'train_runtime': 48.3181, 'train_samples_per_second': 140.486, 'train_steps_per_second': 4.429, 'total_flos': 98230856448120.0, 'train_loss': 0.26252145856340353, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Awesome, training is completed. Let's push it to the Hub.","metadata":{}},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:06:43.220207Z","iopub.execute_input":"2023-11-02T13:06:43.220602Z","iopub.status.idle":"2023-11-02T13:06:45.302002Z","shell.execute_reply.started":"2023-11-02T13:06:43.220569Z","shell.execute_reply":"2023-11-02T13:06:45.300970Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Tirendaz/my_ner_model/tree/main/'"},"metadata":{}}]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Prediction </div></b>","metadata":{}},{"cell_type":"markdown","source":"To inference, let's get a text and classifier its tokens.","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\ntext = \"My name is Sarah, I live in London\"\nclassifier = pipeline(\"ner\", model=\"Tirendaz/my_ner_model\")\nclassifier(text)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:06:45.303456Z","iopub.execute_input":"2023-11-02T13:06:45.304160Z","iopub.status.idle":"2023-11-02T13:06:50.143431Z","shell.execute_reply.started":"2023-11-02T13:06:45.304124Z","shell.execute_reply":"2023-11-02T13:06:50.141763Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996899d9d57f418f917f30f3d449b0bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/266M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa79becc95504ae2aa440829dee321aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac6525968c164d48891035fedcee226a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3559a0d1ff60412faeb1b75c1b79e283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8082235d18f54ef9bc752c2ecd4b5173"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6f6298a94a844778249327154149678"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[{'entity': 'B-person',\n  'score': 0.5418557,\n  'index': 4,\n  'word': 'sarah',\n  'start': 11,\n  'end': 16},\n {'entity': 'B-location',\n  'score': 0.43282637,\n  'index': 9,\n  'word': 'london',\n  'start': 28,\n  'end': 34}]"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see, our model works well. To see predicted labels, let's create a function.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef tag_sentence(text:str):\n    # convert our text to a  tokenized sequence\n    inputs = tokenizer(text, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n    # get outputs\n    outputs = model(**inputs)\n    # convert to probabilities with softmax\n    probs = outputs[0][0].softmax(1)\n    # get the tags with the highest probability\n    word_tags = [(tokenizer.decode(inputs['input_ids'][0][i].item()), id2label[tagid.item()]) \n                  for i, tagid in enumerate (probs.argmax(axis=1))]\n\n    return pd.DataFrame(word_tags, columns=['word', 'tag'])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:06:50.144914Z","iopub.execute_input":"2023-11-02T13:06:50.145240Z","iopub.status.idle":"2023-11-02T13:06:50.155320Z","shell.execute_reply.started":"2023-11-02T13:06:50.145214Z","shell.execute_reply":"2023-11-02T13:06:50.154375Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Now, let's predict tags in text.","metadata":{}},{"cell_type":"code","source":"print(tag_sentence(text))","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:06:50.158255Z","iopub.execute_input":"2023-11-02T13:06:50.158638Z","iopub.status.idle":"2023-11-02T13:06:50.272919Z","shell.execute_reply.started":"2023-11-02T13:06:50.158600Z","shell.execute_reply":"2023-11-02T13:06:50.271566Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"      word         tag\n0    [CLS]           O\n1       my           O\n2     name           O\n3       is           O\n4    sarah    B-person\n5        ,           O\n6        i           O\n7     live           O\n8       in           O\n9   london  B-location\n10   [SEP]           O\n","output_type":"stream"}]},{"cell_type":"markdown","source":"That's it. Thanks for reading. If you enjoy, don't forget to upvote ☺️\n\nLet's connect [YouTube](http://youtube.com/tirendazacademy) | [Medium](http://tirendazacademy.medium.com) | [X](http://x.com/tirendazacademy).\n\nHappy coding 😀","metadata":{}}]}