{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Transformers for TensorFlow</div></b>\n![](https://1.bp.blogspot.com/-qQryqABhdhA/XcC3lJupTKI/AAAAAAAAAzA/MOYu3P_DFRsmNkpjD9j813_SOugPgoBLACLcBGAsYHQ/s1600/h1.png)\n\nThis notebook walks you through how to work with Transformers using TensorFlow.","metadata":{}},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>1. Loading Dataset</div></b>","metadata":{"id":"qV3rP1-P8jtH"}},{"cell_type":"markdown","source":"First, let's install the `datasets` library. ","metadata":{}},{"cell_type":"code","source":"# Installing the \"datasets\" library\n!pip install -q datasets","metadata":{"id":"GMBoR2S9wRwe","outputId":"26305baf-93cd-4de5-9400-6e36274d59a6","execution":{"iopub.status.busy":"2023-09-24T10:33:35.063740Z","iopub.execute_input":"2023-09-24T10:33:35.064422Z","iopub.status.idle":"2023-09-24T10:33:52.555778Z","shell.execute_reply.started":"2023-09-24T10:33:35.064384Z","shell.execute_reply":"2023-09-24T10:33:52.554165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And then let's use this library to load the `rotten_tomatoes` dataset, which presumably contains movie reviews or related data. ","metadata":{}},{"cell_type":"code","source":"# Importing the necessary function to load a dataset\nfrom datasets import load_dataset\n\n# Loading the \"rotten_tomatoes\" dataset\ndataset = load_dataset(\"rotten_tomatoes\")","metadata":{"id":"jaXq6eNB71Bc","outputId":"86a06207-5b06-4931-a728-0fa0f7d9b71a","execution":{"iopub.status.busy":"2023-09-24T10:33:52.560654Z","iopub.execute_input":"2023-09-24T10:33:52.561021Z","iopub.status.idle":"2023-09-24T10:33:57.325178Z","shell.execute_reply.started":"2023-09-24T10:33:52.560990Z","shell.execute_reply":"2023-09-24T10:33:57.323977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's explore this dataset: ","metadata":{}},{"cell_type":"code","source":"# Displaying the loaded dataset\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:33:57.326735Z","iopub.execute_input":"2023-09-24T10:33:57.327872Z","iopub.status.idle":"2023-09-24T10:33:57.338274Z","shell.execute_reply.started":"2023-09-24T10:33:57.327828Z","shell.execute_reply":"2023-09-24T10:33:57.337077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the first example from the \"test\" split of the dataset.","metadata":{}},{"cell_type":"code","source":"# Accessing the first example from the test split of the dataset\ndataset[\"test\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:33:57.342202Z","iopub.execute_input":"2023-09-24T10:33:57.342986Z","iopub.status.idle":"2023-09-24T10:33:57.354605Z","shell.execute_reply.started":"2023-09-24T10:33:57.342947Z","shell.execute_reply":"2023-09-24T10:33:57.353400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>2. Data Preprocessing</div></b>","metadata":{}},{"cell_type":"markdown","source":"Let's initialize for the `distilbert-base-uncased` model. ","metadata":{}},{"cell_type":"code","source":"# Importing the tokenizer for a pre-trained model\nfrom transformers import AutoTokenizer\n\n# Initializing the tokenizer for the \"distilbert-base-uncased\" model\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"id":"cFPjAlkSEspk","execution":{"iopub.status.busy":"2023-09-24T10:33:57.368386Z","iopub.execute_input":"2023-09-24T10:33:57.369159Z","iopub.status.idle":"2023-09-24T10:33:59.790375Z","shell.execute_reply.started":"2023-09-24T10:33:57.369120Z","shell.execute_reply":"2023-09-24T10:33:59.789255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For example, let's tokenize the text of the first training example in the dataset using this tokenizer.","metadata":{}},{"cell_type":"code","source":"# Tokenizing the text of the first training example\ntokenizer(dataset[\"train\"][0][\"text\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:33:59.792041Z","iopub.execute_input":"2023-09-24T10:33:59.792922Z","iopub.status.idle":"2023-09-24T10:33:59.807182Z","shell.execute_reply.started":"2023-09-24T10:33:59.792880Z","shell.execute_reply":"2023-09-24T10:33:59.805842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define a preprocessing function named preprocess_function. This function takes a dictionary of examples as input and tokenizes the \"text\" field with truncation. ","metadata":{}},{"cell_type":"code","source":"# Preprocessing function for tokenization\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)","metadata":{"id":"sr2MTttbEu2I","execution":{"iopub.status.busy":"2023-09-24T10:33:59.808944Z","iopub.execute_input":"2023-09-24T10:33:59.810066Z","iopub.status.idle":"2023-09-24T10:33:59.815373Z","shell.execute_reply.started":"2023-09-24T10:33:59.810028Z","shell.execute_reply":"2023-09-24T10:33:59.814300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that let's use the map function to apply this preprocessing function to the entire dataset in batches.","metadata":{}},{"cell_type":"code","source":"# Applying the preprocessing function to the entire dataset in batches\ndataset = dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:33:59.817143Z","iopub.execute_input":"2023-09-24T10:33:59.817637Z","iopub.status.idle":"2023-09-24T10:34:01.172618Z","shell.execute_reply.started":"2023-09-24T10:33:59.817599Z","shell.execute_reply":"2023-09-24T10:34:01.171483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a batch of examples from the dataset.","metadata":{}},{"cell_type":"code","source":"# Importing the data collator with padding\nfrom transformers import DataCollatorWithPadding\n\n# Initializing the data collator with padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:34:01.176625Z","iopub.execute_input":"2023-09-24T10:34:01.177718Z","iopub.status.idle":"2023-09-24T10:34:11.842804Z","shell.execute_reply.started":"2023-09-24T10:34:01.177647Z","shell.execute_reply":"2023-09-24T10:34:11.841652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>3. Model Loading</div></b>","metadata":{}},{"cell_type":"markdown","source":"Let's load the `distilbert` model for our sentiment analysis.","metadata":{}},{"cell_type":"code","source":"# Importing the TensorFlow version of the model for sequence classification\nfrom transformers import TFAutoModelForSequenceClassification\n\n# Initializing a model for sequence classification using \"distilbert-base-uncased\"\nmy_model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")","metadata":{"id":"it8e8MJX8nEk","outputId":"ddaa5d62-2fd1-46aa-d297-8218fd919b85","execution":{"iopub.status.busy":"2023-09-24T10:34:11.844669Z","iopub.execute_input":"2023-09-24T10:34:11.845618Z","iopub.status.idle":"2023-09-24T10:34:23.888671Z","shell.execute_reply.started":"2023-09-24T10:34:11.845572Z","shell.execute_reply":"2023-09-24T10:34:23.887563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's prepare training and validation datasets as TensorFlow datasets. ","metadata":{}},{"cell_type":"code","source":"# Preparing the training dataset as a TensorFlow dataset\ntf_train_set = my_model.prepare_tf_dataset(\n    dataset[\"train\"],\n    shuffle=True,\n    batch_size=16,\n    collate_fn=data_collator,\n)\n\n# Preparing the validation dataset as a TensorFlow dataset\ntf_validation_set = my_model.prepare_tf_dataset(\n    dataset[\"validation\"],\n    shuffle=False,\n    batch_size=16,\n    collate_fn=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:34:23.890381Z","iopub.execute_input":"2023-09-24T10:34:23.892007Z","iopub.status.idle":"2023-09-24T10:34:24.392844Z","shell.execute_reply.started":"2023-09-24T10:34:23.891966Z","shell.execute_reply":"2023-09-24T10:34:24.391637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>4. Model Training</div></b>","metadata":{}},{"cell_type":"markdown","source":"Let's compile the model using the Adam optimizer with a learning rate of 3e-5.","metadata":{}},{"cell_type":"code","source":"# Importing the Adam optimizer from TensorFlow\nfrom tensorflow.keras.optimizers import Adam\n\n# Compiling the model with the Adam optimizer and a specified learning rate\nmy_model.compile(optimizer=Adam(3e-5))  # No loss argument!","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:34:24.394728Z","iopub.execute_input":"2023-09-24T10:34:24.395566Z","iopub.status.idle":"2023-09-24T10:34:24.783179Z","shell.execute_reply.started":"2023-09-24T10:34:24.395525Z","shell.execute_reply":"2023-09-24T10:34:24.781854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's train the model using the prepared training and validation datasets for 2 epochs.","metadata":{}},{"cell_type":"code","source":"# Training the model\nmy_model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:34:24.784771Z","iopub.execute_input":"2023-09-24T10:34:24.785196Z","iopub.status.idle":"2023-09-24T10:37:05.792461Z","shell.execute_reply.started":"2023-09-24T10:34:24.785154Z","shell.execute_reply":"2023-09-24T10:37:05.791313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>5. Prediction</div></b>","metadata":{}},{"cell_type":"markdown","source":"Let's get a sample text for inference.","metadata":{}},{"cell_type":"code","source":"# Defining a text for inference\ntext = \"I love NLP. It's fun to analyze the NLP tasks with Hugging Face.\"","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:38:16.798563Z","iopub.execute_input":"2023-09-24T10:38:16.799532Z","iopub.status.idle":"2023-09-24T10:38:16.805766Z","shell.execute_reply.started":"2023-09-24T10:38:16.799486Z","shell.execute_reply":"2023-09-24T10:38:16.804487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's preprocess our text for passing our model.","metadata":{}},{"cell_type":"code","source":"# Tokenizing the text for inference\ntokenized_text = tokenizer(text, return_tensors=\"tf\")\ntokenized_text ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:38:29.126330Z","iopub.execute_input":"2023-09-24T10:38:29.127683Z","iopub.status.idle":"2023-09-24T10:38:29.142143Z","shell.execute_reply.started":"2023-09-24T10:38:29.127628Z","shell.execute_reply":"2023-09-24T10:38:29.140784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, let's compute the model's logits (raw output scores) for the tokenized text.","metadata":{}},{"cell_type":"code","source":"# Obtaining model logits for the tokenized text\nlogits = my_model(**tokenized_text).logits","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:40:58.359532Z","iopub.execute_input":"2023-09-24T10:40:58.360136Z","iopub.status.idle":"2023-09-24T10:40:58.562095Z","shell.execute_reply.started":"2023-09-24T10:40:58.360082Z","shell.execute_reply":"2023-09-24T10:40:58.560847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, let's print the index of the class with the highest logit score.","metadata":{}},{"cell_type":"code","source":"# Importing the math module from TensorFlow\nfrom tensorflow import math\n\n# Finding the index of the class with the highest logit score\nint(math.argmax(logits, axis=-1)[0])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T10:48:44.018929Z","iopub.execute_input":"2023-09-24T10:48:44.019751Z","iopub.status.idle":"2023-09-24T10:48:44.028881Z","shell.execute_reply.started":"2023-09-24T10:48:44.019686Z","shell.execute_reply":"2023-09-24T10:48:44.027675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks for reading. If you enjoyed this notebook, don't forget to upvote ‚ò∫Ô∏è\n\nLet's connect [YouTube](http://youtube.com/tirendazacademy) | [Medium](http://tirendazacademy.medium.com) | [Twitter](http://twitter.com/tirendazacademy) | [Linkedin](https://www.linkedin.com/in/tirendaz-academy) üòé","metadata":{}}]}