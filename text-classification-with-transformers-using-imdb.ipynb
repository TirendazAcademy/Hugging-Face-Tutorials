{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load IMDb dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimdb = load_dataset(\"imdb\")","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:00:19.775505Z","iopub.execute_input":"2023-08-21T08:00:19.776103Z","iopub.status.idle":"2023-08-21T08:01:01.205849Z","shell.execute_reply.started":"2023-08-21T08:00:19.776066Z","shell.execute_reply":"2023-08-21T08:01:01.204691Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68098360eada4039a1b90f1976d659f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7c316452a34158bed343ae19ac2894"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19411ba045e04bfbb2e6333f0664fe13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18d0738555cb4bf9bcae83accaa404b3"}},"metadata":{}}]},{"cell_type":"code","source":"imdb","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:01:01.208113Z","iopub.execute_input":"2023-08-21T08:01:01.209015Z","iopub.status.idle":"2023-08-21T08:01:01.217140Z","shell.execute_reply.started":"2023-08-21T08:01:01.208977Z","shell.execute_reply":"2023-08-21T08:01:01.216167Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"imdb[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:01:01.218535Z","iopub.execute_input":"2023-08-21T08:01:01.219118Z","iopub.status.idle":"2023-08-21T08:01:01.232644Z","shell.execute_reply.started":"2023-08-21T08:01:01.219083Z","shell.execute_reply":"2023-08-21T08:01:01.231443Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n 'label': 0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocess\n\nLet's load a DistilBERT tokenizer to preprocess the text field:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:01:01.235154Z","iopub.execute_input":"2023-08-21T08:01:01.235486Z","iopub.status.idle":"2023-08-21T08:01:05.487733Z","shell.execute_reply.started":"2023-08-21T08:01:01.235447Z","shell.execute_reply":"2023-08-21T08:01:05.486599Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b598a268e43468381fb41530554e6cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d10a4136104444937eafea0b2f2eb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"091674826c2b423ebacaad40c84c8ebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0314d5eae7694a4ca2d78f441c02e272"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's create a preprocessing function to tokenize text:","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:01:05.491959Z","iopub.execute_input":"2023-08-21T08:01:05.492987Z","iopub.status.idle":"2023-08-21T08:01:05.498089Z","shell.execute_reply.started":"2023-08-21T08:01:05.492947Z","shell.execute_reply":"2023-08-21T08:01:05.497213Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Let's apply the preprocessing function over the entire dataset:","metadata":{}},{"cell_type":"code","source":"tokenized_imdb = imdb.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:01:05.499643Z","iopub.execute_input":"2023-08-21T08:01:05.500305Z","iopub.status.idle":"2023-08-21T08:02:35.064798Z","shell.execute_reply.started":"2023-08-21T08:01:05.500267Z","shell.execute_reply":"2023-08-21T08:02:35.063481Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea9229c289134499952dcdaa1f5b935b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05a50a59be40447c83294c864ce42e80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4730abdb6e64dcc9abb889dd76c477e"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's use `DataCollatorWithPadding` to perform dynamically pad.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:02:35.066339Z","iopub.execute_input":"2023-08-21T08:02:35.066726Z","iopub.status.idle":"2023-08-21T08:02:54.039092Z","shell.execute_reply.started":"2023-08-21T08:02:35.066688Z","shell.execute_reply":"2023-08-21T08:02:54.038077Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate\n\nLet's use `Evaluate` library to evaluate the model using the `accuracy` metric:","metadata":{}},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:02:54.040605Z","iopub.execute_input":"2023-08-21T08:02:54.041544Z","iopub.status.idle":"2023-08-21T08:03:08.721693Z","shell.execute_reply.started":"2023-08-21T08:02:54.041507Z","shell.execute_reply":"2023-08-21T08:03:08.720180Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\n\naccuracy = evaluate.load(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:03:08.726378Z","iopub.execute_input":"2023-08-21T08:03:08.726768Z","iopub.status.idle":"2023-08-21T08:03:13.051695Z","shell.execute_reply.started":"2023-08-21T08:03:08.726727Z","shell.execute_reply":"2023-08-21T08:03:13.050720Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e6c0ce4037a4199974e2150ce0cafc2"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's create a function to calculate the accuracy:","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:03:13.053200Z","iopub.execute_input":"2023-08-21T08:03:13.053626Z","iopub.status.idle":"2023-08-21T08:03:13.060640Z","shell.execute_reply.started":"2023-08-21T08:03:13.053591Z","shell.execute_reply":"2023-08-21T08:03:13.058822Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Train\n\nFirst, let's create a map of the expected ids to their labels with id2label and label2id:","metadata":{}},{"cell_type":"code","source":"id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\nlabel2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:03:13.064385Z","iopub.execute_input":"2023-08-21T08:03:13.064806Z","iopub.status.idle":"2023-08-21T08:03:13.078083Z","shell.execute_reply.started":"2023-08-21T08:03:13.064756Z","shell.execute_reply":"2023-08-21T08:03:13.077025Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Let's load DistilBERT with AutoModelForSequenceClassification along with the number of expected labels, and the label mappings:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:03:13.079445Z","iopub.execute_input":"2023-08-21T08:03:13.080074Z","iopub.status.idle":"2023-08-21T08:03:17.232413Z","shell.execute_reply.started":"2023-08-21T08:03:13.080026Z","shell.execute_reply":"2023-08-21T08:03:17.231434Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc46e43f3b604330ab6cce5e1a200721"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's define your training hyperparameters in `TrainingArguments`:","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"my_great_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n    report_to = \"none\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:03:17.233890Z","iopub.execute_input":"2023-08-21T08:03:17.234547Z","iopub.status.idle":"2023-08-21T08:03:17.336370Z","shell.execute_reply.started":"2023-08-21T08:03:17.234507Z","shell.execute_reply":"2023-08-21T08:03:17.335219Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Let's pass the training arguments to Trainer along with the model, dataset, tokenizer, data collator, and compute_metrics function.","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_imdb[\"train\"],\n    eval_dataset=tokenized_imdb[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:03:17.340784Z","iopub.execute_input":"2023-08-21T08:03:17.341096Z","iopub.status.idle":"2023-08-21T08:03:25.555597Z","shell.execute_reply.started":"2023-08-21T08:03:17.341069Z","shell.execute_reply":"2023-08-21T08:03:25.554458Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Let's train the model:","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:03:25.557496Z","iopub.execute_input":"2023-08-21T08:03:25.557909Z","iopub.status.idle":"2023-08-21T08:33:35.623162Z","shell.execute_reply.started":"2023-08-21T08:03:25.557870Z","shell.execute_reply":"2023-08-21T08:33:35.622005Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1564' max='1564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1564/1564 30:01, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.278800</td>\n      <td>0.184807</td>\n      <td>0.927680</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.141900</td>\n      <td>0.197168</td>\n      <td>0.930600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1564, training_loss=0.2015788024648681, metrics={'train_runtime': 1809.7255, 'train_samples_per_second': 27.628, 'train_steps_per_second': 0.864, 'total_flos': 6621076590960768.0, 'train_loss': 0.2015788024648681, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference\n\nLet's get a text:","metadata":{}},{"cell_type":"code","source":"text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:33:35.624803Z","iopub.execute_input":"2023-08-21T08:33:35.625238Z","iopub.status.idle":"2023-08-21T08:33:35.632367Z","shell.execute_reply.started":"2023-08-21T08:33:35.625202Z","shell.execute_reply":"2023-08-21T08:33:35.631309Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Let's tokenize this text:","metadata":{}},{"cell_type":"code","source":"inputs = tokenizer(text, return_tensors=\"pt\")\ninputs","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:33:35.633848Z","iopub.execute_input":"2023-08-21T08:33:35.634494Z","iopub.status.idle":"2023-08-21T08:33:35.647611Z","shell.execute_reply.started":"2023-08-21T08:33:35.634454Z","shell.execute_reply":"2023-08-21T08:33:35.646685Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  2023,  2001,  1037, 17743,  1012,  2025,  3294, 11633,  2000,\n          1996,  2808,  1010,  2021,  4372,  2705,  7941,  2989,  2013,  2927,\n          2000,  2203,  1012,  2453,  2022,  2026,  5440,  1997,  1996,  2093,\n          1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's load the model and predict the label:","metadata":{}},{"cell_type":"code","source":"import torch\nmodel_path = \"/kaggle/working/my_great_model/checkpoint-1564\"\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_path, num_labels=2)\n\nwith torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:34:23.126827Z","iopub.execute_input":"2023-08-21T08:34:23.127208Z","iopub.status.idle":"2023-08-21T08:34:24.349834Z","shell.execute_reply.started":"2023-08-21T08:34:23.127176Z","shell.execute_reply":"2023-08-21T08:34:24.348593Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the prediction:","metadata":{}},{"cell_type":"code","source":"predicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]","metadata":{"execution":{"iopub.status.busy":"2023-08-21T08:34:29.047363Z","iopub.execute_input":"2023-08-21T08:34:29.047907Z","iopub.status.idle":"2023-08-21T08:34:29.065947Z","shell.execute_reply.started":"2023-08-21T08:34:29.047859Z","shell.execute_reply":"2023-08-21T08:34:29.064843Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'POSITIVE'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Resources\n\n- [Text classification](https://huggingface.co/docs/transformers/tasks/sequence_classification)","metadata":{}}]}