{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1: Install Packages and Import Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -q seqeval\n!pip install -q transformers\n!pip install -q datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-19T07:38:58.382257Z","iopub.execute_input":"2023-08-19T07:38:58.382645Z","iopub.status.idle":"2023-08-19T07:39:36.680257Z","shell.execute_reply.started":"2023-08-19T07:38:58.382611Z","shell.execute_reply":"2023-08-19T07:39:36.678730Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\nfrom datasets import load_dataset, load_metric, Dataset, DatasetDict\nimport numpy as np\nfrom seqeval.metrics import f1_score, precision_score, recall_score, classification_report","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:36.683944Z","iopub.execute_input":"2023-08-19T07:39:36.684647Z","iopub.status.idle":"2023-08-19T07:39:52.071073Z","shell.execute_reply.started":"2023-08-19T07:39:36.684618Z","shell.execute_reply":"2023-08-19T07:39:52.070115Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 2: Check CUDA Availability and Device Information","metadata":{}},{"cell_type":"code","source":"print(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Current device index:\", torch.cuda.current_device())\nprint(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.072560Z","iopub.execute_input":"2023-08-19T07:39:52.073361Z","iopub.status.idle":"2023-08-19T07:39:52.167320Z","shell.execute_reply.started":"2023-08-19T07:39:52.073333Z","shell.execute_reply":"2023-08-19T07:39:52.166228Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"CUDA available: True\nCurrent device index: 0\nDevice name: Tesla T4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 3: Read and Prepare Data","metadata":{}},{"cell_type":"code","source":"def read_conll_file(file_path):\n    with open(file_path, \"r\") as f:\n        content = f.read().strip()\n        sentences = content.split(\"\\n\\n\")\n        data = []\n        for sentence in sentences:\n            tokens = sentence.split(\"\\n\")\n            token_data = []\n            for token in tokens:\n                token_data.append(token.split())\n            data.append(token_data)\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.170227Z","iopub.execute_input":"2023-08-19T07:39:52.170562Z","iopub.status.idle":"2023-08-19T07:39:52.176462Z","shell.execute_reply.started":"2023-08-19T07:39:52.170536Z","shell.execute_reply":"2023-08-19T07:39:52.175375Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data = read_conll_file(\"/kaggle/input/conll2003-dataset/conll2003/eng.testa\")\nvalidation_data = read_conll_file(\"/kaggle/input/conll2003-dataset/conll2003/eng.testa\")\ntest_data = read_conll_file(\"/kaggle/input/conll2003-dataset/conll2003/eng.testb\")","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.178057Z","iopub.execute_input":"2023-08-19T07:39:52.178397Z","iopub.status.idle":"2023-08-19T07:39:52.683431Z","shell.execute_reply.started":"2023-08-19T07:39:52.178366Z","shell.execute_reply":"2023-08-19T07:39:52.682287Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data[:2]","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.685013Z","iopub.execute_input":"2023-08-19T07:39:52.685514Z","iopub.status.idle":"2023-08-19T07:39:52.697354Z","shell.execute_reply.started":"2023-08-19T07:39:52.685471Z","shell.execute_reply":"2023-08-19T07:39:52.696004Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[[['-DOCSTART-', '-X-', '-X-', 'O']],\n [['CRICKET', 'NNP', 'B-NP', 'O'],\n  ['-', ':', 'O', 'O'],\n  ['LEICESTERSHIRE', 'NNP', 'B-NP', 'B-ORG'],\n  ['TAKE', 'NNP', 'I-NP', 'O'],\n  ['OVER', 'IN', 'B-PP', 'O'],\n  ['AT', 'NNP', 'B-NP', 'O'],\n  ['TOP', 'NNP', 'I-NP', 'O'],\n  ['AFTER', 'NNP', 'I-NP', 'O'],\n  ['INNINGS', 'NNP', 'I-NP', 'O'],\n  ['VICTORY', 'NN', 'I-NP', 'O'],\n  ['.', '.', 'O', 'O']]]"},"metadata":{}}]},{"cell_type":"code","source":"print(len(train_data))\nprint(len(validation_data))\nprint(len(test_data))","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.699203Z","iopub.execute_input":"2023-08-19T07:39:52.700154Z","iopub.status.idle":"2023-08-19T07:39:52.707304Z","shell.execute_reply.started":"2023-08-19T07:39:52.700118Z","shell.execute_reply":"2023-08-19T07:39:52.706002Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"3466\n3466\n3684\n","output_type":"stream"}]},{"cell_type":"code","source":"def convert_to_dataset(data, label_map):\n    formatted_data = {\"tokens\": [], \"ner_tags\": []}\n    for sentence in data:\n        tokens = [token_data[0] for token_data in sentence]\n        ner_tags = [label_map[token_data[3]] for token_data in sentence]\n        formatted_data[\"tokens\"].append(tokens)\n        formatted_data[\"ner_tags\"].append(ner_tags)\n    return Dataset.from_dict(formatted_data)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.709000Z","iopub.execute_input":"2023-08-19T07:39:52.709672Z","iopub.status.idle":"2023-08-19T07:39:52.720471Z","shell.execute_reply.started":"2023-08-19T07:39:52.709625Z","shell.execute_reply":"2023-08-19T07:39:52.719383Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"label_list = sorted(list(set([token_data[3] for sentence in train_data for token_data in sentence])))\nlabel_map = {label: i for i, label in enumerate(label_list)}","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.721833Z","iopub.execute_input":"2023-08-19T07:39:52.722305Z","iopub.status.idle":"2023-08-19T07:39:52.738034Z","shell.execute_reply.started":"2023-08-19T07:39:52.722271Z","shell.execute_reply":"2023-08-19T07:39:52.736874Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset = convert_to_dataset(train_data, label_map)\nvalidation_dataset = convert_to_dataset(validation_data, label_map)\ntest_dataset = convert_to_dataset(test_data, label_map)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.742560Z","iopub.execute_input":"2023-08-19T07:39:52.742843Z","iopub.status.idle":"2023-08-19T07:39:52.882512Z","shell.execute_reply.started":"2023-08-19T07:39:52.742819Z","shell.execute_reply":"2023-08-19T07:39:52.881449Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.883845Z","iopub.execute_input":"2023-08-19T07:39:52.884221Z","iopub.status.idle":"2023-08-19T07:39:52.893450Z","shell.execute_reply.started":"2023-08-19T07:39:52.884188Z","shell.execute_reply":"2023-08-19T07:39:52.892380Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['tokens', 'ner_tags'],\n    num_rows: 3466\n})"},"metadata":{}}]},{"cell_type":"code","source":"datasets = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": validation_dataset,\n    \"test\": test_dataset,\n})","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.895205Z","iopub.execute_input":"2023-08-19T07:39:52.895689Z","iopub.status.idle":"2023-08-19T07:39:52.903638Z","shell.execute_reply.started":"2023-08-19T07:39:52.895655Z","shell.execute_reply":"2023-08-19T07:39:52.902629Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Initialize Tokenizer and Model","metadata":{}},{"cell_type":"code","source":"model_name = \"bert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:52.904938Z","iopub.execute_input":"2023-08-19T07:39:52.905498Z","iopub.status.idle":"2023-08-19T07:39:57.513602Z","shell.execute_reply.started":"2023-08-19T07:39:52.905444Z","shell.execute_reply":"2023-08-19T07:39:57.512541Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f8322f34bf4e18ba032fd7ff87afdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78e2770bb938476c9d1e1101a383617a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64782dce8985448b90ab8b0ee69b2dae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"979c6d7cf3354c2987502c5f756fca62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7a1b5fe706c492c8899a753e44f9a32"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 5: Define Metrics and Tokenization Function","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_prediction):\n    predictions, labels = eval_prediction\n    predictions = np.argmax(predictions, axis=2)\n\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n\n    return {\n        \"precision\": precision_score(true_labels, true_predictions),\n        \"recall\": recall_score(true_labels, true_predictions),\n        \"f1\": f1_score(true_labels, true_predictions),\n        \"classification_report\": classification_report(true_labels, true_predictions),\n    }","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:57.515056Z","iopub.execute_input":"2023-08-19T07:39:57.516087Z","iopub.status.idle":"2023-08-19T07:39:57.525907Z","shell.execute_reply.started":"2023-08-19T07:39:57.516044Z","shell.execute_reply":"2023-08-19T07:39:57.524437Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True\n    )\n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:57.527482Z","iopub.execute_input":"2023-08-19T07:39:57.527840Z","iopub.status.idle":"2023-08-19T07:39:57.538777Z","shell.execute_reply.started":"2023-08-19T07:39:57.527804Z","shell.execute_reply":"2023-08-19T07:39:57.537623Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Tokenize Datasets and Set Training Arguments","metadata":{}},{"cell_type":"code","source":"tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_steps=500,\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_steps=100,\n    learning_rate=5e-5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:39:57.540156Z","iopub.execute_input":"2023-08-19T07:39:57.540670Z","iopub.status.idle":"2023-08-19T07:40:00.188368Z","shell.execute_reply.started":"2023-08-19T07:39:57.540635Z","shell.execute_reply":"2023-08-19T07:40:00.186957Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0761c077b364bfb821f46c6a613e6be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d025f79d7a4485b90b2fff1434885c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf2fa0c5d254b8183e6927a7a6c60df"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Step 7: Define Data Collator and Initialize Trainer","metadata":{}},{"cell_type":"code","source":"def data_collator(data):\n    input_ids = [torch.tensor(item[\"input_ids\"]) for item in data]\n    attention_mask = [torch.tensor(item[\"attention_mask\"]) for item in data]\n    labels = [torch.tensor(item[\"labels\"]) for item in data]\n\n\n    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels,\n    }","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:40:00.190743Z","iopub.execute_input":"2023-08-19T07:40:00.191576Z","iopub.status.idle":"2023-08-19T07:40:00.200824Z","shell.execute_reply.started":"2023-08-19T07:40:00.191537Z","shell.execute_reply":"2023-08-19T07:40:00.199713Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:40:00.202460Z","iopub.execute_input":"2023-08-19T07:40:00.202984Z","iopub.status.idle":"2023-08-19T07:40:05.893405Z","shell.execute_reply.started":"2023-08-19T07:40:00.202856Z","shell.execute_reply":"2023-08-19T07:40:05.891483Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Step 8: Train the Model","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:40:05.900020Z","iopub.execute_input":"2023-08-19T07:40:05.900748Z","iopub.status.idle":"2023-08-19T07:42:18.668786Z","shell.execute_reply.started":"2023-08-19T07:40:05.900711Z","shell.execute_reply":"2023-08-19T07:42:18.666494Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230819_074029-8klrp4m6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tirendaz-academy/huggingface/runs/8klrp4m6' target=\"_blank\">fancy-monkey-3</a></strong> to <a href='https://wandb.ai/tirendaz-academy/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tirendaz-academy/huggingface' target=\"_blank\">https://wandb.ai/tirendaz-academy/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tirendaz-academy/huggingface/runs/8klrp4m6' target=\"_blank\">https://wandb.ai/tirendaz-academy/huggingface/runs/8klrp4m6</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='217' max='217' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [217/217 01:10, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=217, training_loss=0.17748494219670097, metrics={'train_runtime': 132.2631, 'train_samples_per_second': 26.205, 'train_steps_per_second': 1.641, 'total_flos': 266656946009220.0, 'train_loss': 0.17748494219670097, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"sentence = \"John Smith is a software engineer who works at Google.\"\ntokenized_input = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\noutputs = model(**tokenized_input)\npredicted_labels = outputs.logits.argmax(-1)[0]\nnamed_entities = [tokenizer.decode([token]) for token, label in zip(tokenized_input[\"input_ids\"][0], predicted_labels) if label != 0 and label != label_map['O']]\nprint(\"Named Entities - Example 1:\", named_entities)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:42:18.670496Z","iopub.execute_input":"2023-08-19T07:42:18.671734Z","iopub.status.idle":"2023-08-19T07:42:18.725860Z","shell.execute_reply.started":"2023-08-19T07:42:18.671691Z","shell.execute_reply":"2023-08-19T07:42:18.723963Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Named Entities - Example 1: ['John', 'Smith', 'Google']\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence2 = \"The company Apple Inc. announced its new product, the iPhone 12, at a press conference held in San Francisco.\"\ntokenized_input2 = tokenizer(sentence2, return_tensors=\"pt\").to(model.device)\noutputs2 = model(**tokenized_input2)\npredicted_labels2 = outputs2.logits.argmax(-1)[0]\nnamed_entities2 = [tokenizer.decode([token]) for token, label in zip(tokenized_input2[\"input_ids\"][0], predicted_labels2) if label != 0 and label != label_map['O']]\nprint(\"Named Entities - Example 2:\", named_entities2)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T07:42:18.730634Z","iopub.execute_input":"2023-08-19T07:42:18.733172Z","iopub.status.idle":"2023-08-19T07:42:18.775268Z","shell.execute_reply.started":"2023-08-19T07:42:18.733133Z","shell.execute_reply":"2023-08-19T07:42:18.774174Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Named Entities - Example 2: ['Apple', 'Inc', 'iPhone', '12', 'Francisco']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Resources\n- [Named Entity Recognition With HuggingFace Using PyTorch and W&B]()","metadata":{}}]}